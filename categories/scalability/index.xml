<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> </title>
    <link>https://www.xaprb.com/categories/scalability/index.xml</link>
    <language>en-us</language>
    <author></author>
    <rights>Copyright (c) 2016</rights>
    <updated>0001-01-01 00:00:00 &#43;0000 UTC</updated>

    
        <item>
          <title>The Response Time Stretch Factor</title>
          <link>https://www.xaprb.com/blog/response-time-stretch-factor/</link>
          <pubDate>Sun, 30 Oct 2016 10:00:39 -0400</pubDate>
          <author></author>
          <guid>https://www.xaprb.com/blog/response-time-stretch-factor/</guid>
          <description>&lt;p&gt;Computer systems, and for that matter all types of systems that receive requests
and process them, have a response time that includes some time waiting in queue
if the server is busy when a request arrives. The wait time increases sharply as
the server gets busier. For simple M/M/m systems there is a simple equation that
describes this exactly, but for more complicated systems this equation is only
approximate. This has rattled around in my brain for a long time, and rather
than keeping my notes private I&amp;rsquo;m sharing them here (although since I&amp;rsquo;m still
trying to learn this stuff I may just be putting my ignorance on full display).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.xaprb.com/media/2016/10/hockey-stick.png&#34; alt=&#34;Hockey-Stick Curve&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll skip all the derivations and go through only the basics to get to the results.&lt;/p&gt;

&lt;h3 id=&#34;the-stretch-factor-heuristic-and-exact-formula&#34;&gt;The Stretch Factor Heuristic and Exact Formula&lt;/h3&gt;

&lt;p&gt;The equation describing the curve above is derived as follows. When a request
arrives at the server, it will have to wait time \(S\), the service time, to be
completed. But if the server is busy, which happens with some probability, then
it will be delayed some additional wait time \(W\) for the request in process,
and potentially for other requests are already waiting, before it enters
service. The total time is the residence time, \(R = W + S\).&lt;/p&gt;

&lt;p&gt;The wait time \(W\) is some fraction of the total residence time, which can be
at most 100%, and if this is denoted by \(\rho\) then \(W = \rho R\).
Thus,&lt;/p&gt;

&lt;p&gt;$$
R = W + S = \rho R + S
$$&lt;/p&gt;

&lt;p&gt;Which, when rearranged and divided by service time to make it a relative
&amp;ldquo;stretch factor,&amp;rdquo; becomes&lt;/p&gt;

&lt;p&gt;$$
R = \frac{1}{1- \rho}
$$&lt;/p&gt;

&lt;p&gt;None of this is original; I&amp;rsquo;ve basically cribbed this from Neil Gunther&amp;rsquo;s book
&lt;em&gt;Analyzing Computer Systems Performance with Perl::PDQ&lt;/em&gt;. Later in that same
book, Gunther shows the derivation of a related formula for the stretch factor
in a multiserver queue with \(m\) servers,&lt;/p&gt;

&lt;p&gt;$$
R \approx \frac{1}{1-\rho^m}
$$&lt;/p&gt;

&lt;p&gt;Where \(\rho\) denotes server utilization. This heuristic approximation does
arise analytically, but is not exact when there are more than 2 servers. It
underestimates wait time when utilization is high, especially with large numbers
of servers (say, 64). The exact solution is given by&lt;/p&gt;

&lt;p&gt;$$
R = \frac{C(m, \rho) S}{m(1-\rho)} + S
$$&lt;/p&gt;

&lt;p&gt;Where the first term is just \(W\), and the function \(C(m,\rho)\) is the
Erlang C function. If we put it all together and rearrange into &amp;ldquo;stretch factor&amp;rdquo; form, the Erlang equation for stretch factor is&lt;/p&gt;

&lt;p&gt;$$
R(m, \rho) = 1 + \frac{ \frac{(m \rho)^m}{m!} }{ (1-\rho) \sum_{n=0}^{m-1} \frac{(m \rho)^n}{n!} + \frac{(m \rho)^m}{m!} } \frac{1}{m(1-\rho)}
$$&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;d like to see how this looks, check out &lt;a href=&#34;https://www.desmos.com/calculator/9dr7azq0ot&#34;&gt;this Desmos calculator&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now here&amp;rsquo;s the part my brain has been trying to connect, almost idly in
shower-thought time, for a while. If intuition led from one direction to the
heuristic approximation \(R=\frac{1}{1-\rho^m}\), and an exact derivation leads to
a formula that in the single- and dual-server case is the same as the
heuristic, then what is the heuristic missing to extend to an exact multiserver
queueing system? Can it be extended, or is it just a coincidence that it&amp;rsquo;s an
exact solution for \(m=1, m=2\)?&lt;/p&gt;

&lt;p&gt;A few trains of thought have sprung into my mind.&lt;/p&gt;

&lt;p&gt;First, I observed that the Erlang C formula includes \(m!\), and it happens to be the
case that 1! = 1, and 2! = 2. A clue, or a distraction? What if I add a term
to the heuristic, multiplied by \(m/m!\) or similar, which would just reduce
to the same thing for the single- and dual-server cases? What form would that
term take?&lt;/p&gt;

&lt;p&gt;Secondly, what if I work backwards from the Erlang formula and see if it reduces
to a different form of the heuristic?&lt;/p&gt;

&lt;p&gt;Thirdly, what if the heuristic&amp;rsquo;s exactness for the base cases is just a
coincidence after all? In that case, perhaps a new, simpler approximation to the
Erlang formula is waiting to be invented. Approximations are highly useful to me
in tools such as spreadsheets and the like, or even in using intuition, which is
workable with the heuristic form. Approximations are easy to think about and a
lot easier to type and troubleshoot. They&amp;rsquo;re also faster to compute, should
performance matter.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll take each of these cases in turn.&lt;/p&gt;

&lt;h3 id=&#34;a-missing-term&#34;&gt;A Missing Term&lt;/h3&gt;

&lt;p&gt;If the heuristic is exact for 1- and 2-server cases because of the coincidence
that the factorial function for these values is the value itself, then what is
the missing term? What shape might it have?&lt;/p&gt;

&lt;p&gt;To gain some intuition about this, I wrote a simple &lt;a href=&#34;https://www.desmos.com/calculator/qo1n4shf1f&#34;&gt;Desmos
calculator&lt;/a&gt; to show
the &lt;em&gt;value&lt;/em&gt; of the hypothetical &amp;ldquo;missing term&amp;rdquo; in the context of the heuristic&amp;rsquo;s
value. In other words, the heuristic&amp;rsquo;s &lt;em&gt;error function&lt;/em&gt;.  Here&amp;rsquo;s a picture of
that for several values of \(m\). Red is 1 server, green is 8, purple is 16, orange
is 64.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.xaprb.com/media/2016/10/error-in-heuristic.png&#34; alt=&#34;Error in Heuristic&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note that I showed utilization extending out beyond the value 1 because the
shape of the function is interesting, but that value is impossible&amp;mdash;a server can
never be more than 100% utilized. What&amp;rsquo;s interesting, to me anyway, is that the
error function looks kind of like the PDF of a Gamma distribution. I&amp;rsquo;ll leave that thought
there.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m not sure what form a term including, say, \(m/m!\) would take. This is
something I haven&amp;rsquo;t explored a lot yet. &lt;em&gt;TODO&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&#34;reducing-erlang&#34;&gt;Reducing Erlang&lt;/h3&gt;

&lt;!-- Wolfram Alpha input

1 + ((m p)^m/m!)/((1-p) Sum[(m p)^n/n!, {n, 0, m-1}] + (m p)^m/m!) * 1/(m(1-p)), m=1

--&gt;

&lt;p&gt;What does the Erlang formula reduce to in the \(m=1\) case? The Erlang C
formula itself reduces to \(\rho\), and when
decorated with the additional stuff to get it into stretch-factor form, it
reduces to&lt;/p&gt;

&lt;p&gt;$$
R(1, \rho) = 1 + \frac{\rho}{1-\rho}
$$&lt;/p&gt;

&lt;p&gt;Which is just a rearrangement of the heuristic function.
(Interestingly, Wolfram Alpha will simplify it to
include the Gamma function and list the heuristic as an approximation. Gamma
again, though Gamma function and Gamma distribution are different. The Gamma
function is closely related to the factorial function.)&lt;/p&gt;

&lt;p&gt;In the \(m=2\) case, if I&amp;rsquo;m doing my algebra right, the Erlang C function
simplifies to&lt;/p&gt;

&lt;p&gt;$$
C(2, \rho) = \frac{2\rho^2}{ (1-\rho) + (1-\rho)2\rho + 2\rho^2}
$$&lt;/p&gt;

&lt;p&gt;Which further simplifies to \(\frac{2\rho^2}{\rho+1}\), which when
rewritten into stretch-factor form, becomes&lt;/p&gt;

&lt;p&gt;$$
R(2, \rho)=1+\frac{2\rho^2}{\rho+1} \frac{1}{2(1-\rho)}
$$&lt;/p&gt;

&lt;p&gt;Which is exactly \(\frac{1}{1-\rho^2}\), the heuristic form.&lt;/p&gt;

&lt;p&gt;At \(m=3\) and above, the heuristic is only approximate. What does the Erlang
form reduce to for the first of those cases? Does it result in the missing term
that will extend to 4 and beyond too? (&lt;em&gt;Note: I wrote about this in a &lt;a href=&#34;https://www.xaprb.com/blog/erlang-stretch-factor-three-four/&#34;&gt;followup
post&lt;/a&gt;&lt;/em&gt;).&lt;/p&gt;

&lt;h3 id=&#34;approximations-to-erlang-based-on-the-heuristic&#34;&gt;Approximations to Erlang, Based on the Heuristic&lt;/h3&gt;

&lt;p&gt;Approximations to complicated equations are often really useful. You use them
all the time, although you may not know it. For example, a computer uses
approximations to calculate functions such as sine, square root, and logarithm.
The field of numeric methods is dedicated to studying such algorithms and their
errors.&lt;/p&gt;

&lt;p&gt;In 1977, Sakasegawa discovered an approximation to the length of a queue, which
is more accurate than the heuristic function, but wasn&amp;rsquo;t derived analytically.
You can find the paper
&lt;a href=&#34;https://github.com/VividCortex/approx-queueing-theory&#34;&gt;here&lt;/a&gt;. In a nutshell,
his method was to begin with a well-known queueing theory formula derived by
&lt;a href=&#34;https://en.wikipedia.org/wiki/Pollaczek%E2%80%93Khinchine_formula&#34;&gt;Pollaczek and
Khinchine&lt;/a&gt;,
which is valid (exact) for M/G/1 systems, that is, single-server queueing
systems in some specific cases. Sakasegawa observed the error curve at various
parameters and essentially did a least-squares sum of errors regression to
arrive at an approximation for the queue length, which in the simplest types of
queueing systems reduces to:&lt;/p&gt;

&lt;p&gt;$$
L_q \approx \frac{ \rho^{\sqrt{2(m+1)}} }{ 1-\rho}
$$&lt;/p&gt;

&lt;p&gt;In &amp;ldquo;stretch factor&amp;rdquo; form, this becomes&lt;/p&gt;

&lt;p&gt;$$
R(m, \rho) \approx 1+\frac{\rho^{\sqrt{2(m+1)}}}{\rho m(1-\rho)}
$$&lt;/p&gt;

&lt;p&gt;This is such an accurate approximation that it&amp;rsquo;s more than good enough for
real-life applications, and I use it all the time. (It&amp;rsquo;s hard to put Erlang&amp;rsquo;s
formula into a spreadsheet, because it has iterative computations.)&lt;/p&gt;

&lt;p&gt;Given the usefulness and simplicity of approximations, perhaps an approximation
to the stretch factor can be derived from the heuristic form
\(1/(1-\rho^m)\). This idea appeals to me because it might lead to insights
about what&amp;rsquo;s disappeared or simplified out of the exact Erlang formula in the
base case.&lt;/p&gt;

&lt;p&gt;One possible way to do this is to approximate the error function of the
heuristic.  I already mentioned that it might be possible to do this with the
Gamma distribution&amp;rsquo;s PDF. Finding an approximation to that, and then adding or
multiplying by it, might result in a usable approximation.&lt;/p&gt;

&lt;p&gt;Another idea is a sigmoid such as the classic logistic function. In fact, if I
wanted to approximate the error in the range (0,1) this isn&amp;rsquo;t too bad an
approximation for \(m=16\), for example:&lt;/p&gt;

&lt;p&gt;$$
\frac{.5}{1+e^{-10(x-1)}}
$$&lt;/p&gt;

&lt;p&gt;Finally, instead of adding a term or multiplying the heuristic by a term,
perhaps it&amp;rsquo;s the \(\rho^m\) portion in the denominator that needs to be
tweaked. A little analysis led me to the following conclusions.&lt;/p&gt;

&lt;p&gt;The error at \(m&amp;gt;3\) could be explained by the exponent \(m\) being too large. If so,
then the correct value for the exponent could be a function of \(\rho\), and
an adjustment to it would need to be of the form&lt;/p&gt;

&lt;p&gt;$$
A_{exp} = \frac{log\left( \frac{E(\rho)-1}{E(\rho)} \right)}{log(\rho)}
$$&lt;/p&gt;

&lt;p&gt;Where \(E(\rho)\) is the Erlang formula for the stretch factor. I arrived at
this by solving the error function for \(\rho\). For convenience, this can be
divided by \(m\) to normalize it relative to the number of servers. I&amp;rsquo;ve made
a &lt;a href=&#34;https://www.desmos.com/calculator/7ygut81via&#34;&gt;Desmos calculator&lt;/a&gt; illustrating
the shape of this adjustment term for 1, 4, 8, and 16 servers:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.xaprb.com/media/2016/10/heuristic-error-func-util.png&#34; alt=&#34;Error in Heuristic as Func of Util&#34; /&gt;&lt;/p&gt;

&lt;p&gt;One of the challenges with this is that due to the limitations of floating-point
math in computers, the heuristic function appears to have no error at low
utilization. I think it does, but it&amp;rsquo;s just a small value. That&amp;rsquo;s why the shape
of that curve has a discontinuity at low utilization.&lt;/p&gt;

&lt;p&gt;An interesting observation: If you zoom out and remove the limits on the range
of values plotted, you get something that looks like a part of the Gamma
function. Is this a coincidence? Could the Gamma function be used as an
approximation to this error? Or is the missing term a Gamma function, which
would result in an exact solution? &lt;em&gt;TODO&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Another way to nudge the heuristic to approximate the Erlang residence time
stretch factor would be to examine whether the base, \(\rho\), is too large
or too small. Following a similar train of thought as before and solving the
error function for the number of servers, I found that the error would need to be of the
form&lt;/p&gt;

&lt;p&gt;$$
A_{base} = \left( \frac{E(\rho)-1}{E(\rho)}\right)^{1/m}
$$&lt;/p&gt;

&lt;p&gt;Normalizing this relative to \(\rho\) by dividing, I got a function that has
similar discontinuities as before, and is of the following shape. It looks
like it might be possible to approximate with something like a quadratic from 0
to 1, but if you zoom out further, it looks more like&amp;hellip; wait for it&amp;hellip; part of
the Gamma function. You can see this on
&lt;a href=&#34;https://www.desmos.com/calculator/hsidkl4og8&#34;&gt;Desmos&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.xaprb.com/media/2016/10/heuristic-error-func-servers.png&#34; alt=&#34;Heuristic Error as Func of Servers&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I experimented with this in a different way, by trying to approximate
\(A_{base}\) directly. I just guessed at its shape and came up with the
following, which isn&amp;rsquo;t too far off for \(2&amp;lt;m&amp;lt;5\):&lt;/p&gt;

&lt;p&gt;$$
A_{base} \approx \rho - \frac{2}{15} \sqrt{m} (\rho-1)\rho
$$&lt;/p&gt;

&lt;p&gt;You can see this shape, compared with the actual \(A_{base}\), at this
&lt;a href=&#34;https://www.desmos.com/calculator/sgwrqdcnzk&#34;&gt;Desmos&lt;/a&gt;. And &lt;a href=&#34;https://www.desmos.com/calculator/opa1sfpxfw&#34;&gt;this
one&lt;/a&gt; shows what this looks like
when included as a term in the heuristic stretch factor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.xaprb.com/media/2016/10/heuristic-skew-util.png&#34; alt=&#34;Heuristic Skewed By Utilization&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Red is Erlang, blue is my heuristic, and black dashed is Gunther&amp;rsquo;s.
Don&amp;rsquo;t be fooled; mine may look better, but if you examine high utilizations
you&amp;rsquo;ll see it&amp;rsquo;s much worse. Small errors in the approximation to the error
function make big differences in the result. (This is true if the error is
defined as a function of number of servers, too.)&lt;/p&gt;

&lt;h3 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;There&amp;rsquo;s a lot of outright superstition in this blog post, but hopefully some of
the intuition I&amp;rsquo;m trying to develop comes through as well. Guessing at
approximations (and seeing Gamma-things everywhere, like shapes in the clouds)
is probably more time-wasting than productive, but I find that by playing with
things I learn a lot. There&amp;rsquo;s something still unfinished in my mind, something
unsatisfactory about coming towards an exact solution from two directions,
finding two different forms, and finding that one of them runs aground at a
certain point, even though it&amp;rsquo;s just a simplification of the other under some
circumstances. You might replace &amp;ldquo;even though&amp;rdquo; with &amp;ldquo;because&amp;rdquo; in the previous
sentence, and be satisfied, but as for me I feel there&amp;rsquo;s still something to be
learned here. And potentially a useful result might come out of it, even if
it&amp;rsquo;s only a fast approximation like Sakasegawa&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;Besides, I just enjoy exploring math and its shapes; this is a great form of
relaxation or stress relief for me when I want to concentrate on something so as
to put other things out of my mind. Hopefully there&amp;rsquo;s a kindred soul out there
who finds this interesting too. If so, hello, and enjoy!&lt;/p&gt;</description>
        </item>
    
        <item>
          <title>How to Extract Data Points From a Chart</title>
          <link>https://www.xaprb.com/blog/2015/12/06/get-data-points-from-chart/</link>
          <pubDate>Sun, 06 Dec 2015 19:38:48 -0500</pubDate>
          <author></author>
          <guid>https://www.xaprb.com/blog/2015/12/06/get-data-points-from-chart/</guid>
          <description>&lt;p&gt;I often see benchmark reports that show charts but don&amp;rsquo;t provide tables of
numeric results. Some people will make the actual measurements available if
asked, but I&amp;rsquo;ve been interested in
&lt;a href=&#34;https://www.vividcortex.com/resources/universal-scalability-law/&#34;&gt;analyzing&lt;/a&gt;
many systems for which I can&amp;rsquo;t get numbers.  Fortunately, it&amp;rsquo;s usually possible
to get approximate results without too much trouble. In this blog post I&amp;rsquo;ll show
several ways to extract estimates of values from a chart image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.xaprb.com/media/2015/12/espresso.jpg&#34; alt=&#34;Extracting&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll motivate the idea with a simple chart of a slide I saw at the recent
PGConfSV conference on a keynote presentation. I was interested in the benchmark
data (for &lt;a href=&#34;https://www.citusdata.com/&#34;&gt;CitusDB&lt;/a&gt;, in this case) but I am sure
they are busy and I haven&amp;rsquo;t gotten in touch with them yet about it. So I watched
the YouTube video of the keynote address, paused it when the slide was showing,
and took a screenshot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.xaprb.com/media/2015/12/citusdb-benchmark.jpg&#34; alt=&#34;CitusDB Benchmark Result&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d like to use the Universal Scalability Law to analyze the chart on the left
and estimate how much of CitusDB&amp;rsquo;s massively parallel processing is serialized.
I am not an expert on it, but I believe it uses a PostgreSQL node with a plugin
as a query planner and router, sends queries to shards containing data, and
combines the results to return them to the client. This is fairly standard
scatter-gather processing. One of the big limiting factors for such a system is
typically the query planning and the merging of the results. How much of the
overall execution time does that take? The Universal Scalability Law can help
understand that. But to analyze the system, first I need its data.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see how to get it.&lt;/p&gt;

&lt;h3 id=&#34;doing-it-the-hard-way&#34;&gt;Doing It The Hard Way&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;m going to show you two hard ways to do this and suggest a couple of easier
ways.&lt;/p&gt;

&lt;p&gt;One is to use any photo editing software and a ruler or crop function to
estimate the pixel center of the points on the chart. For example, here&amp;rsquo;s a
screen capture of dragging from the bottom left of the image to the bottom left
of the chart to get an X-Y point for the intersection of the chart&amp;rsquo;s X and Y
axes. I&amp;rsquo;m using the default Mac image editing program here, Preview:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.xaprb.com/media/2015/12/using-preview.jpg&#34; alt=&#34;Using Preview To Estimate Points&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The result is X=326,Y=183. Do this repeatedly for all the points on the chart
and then put the results into a spreadsheet, subtract the origin X and Y from
all the points, and normalize them relative to known points on the chart axes
(which you should also measure) and you&amp;rsquo;ve got your results. Here&amp;rsquo;s a screenshot
of the spreadsheet I made to do this. The points I measured are in green and the
results are in purple:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.xaprb.com/media/2015/12/points-spreadsheet.png&#34; alt=&#34;Spreadsheet to calculate results&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You may &lt;a href=&#34;https://www.xaprb.com/media/2015/12/spreadsheet.xlsx&#34;&gt;download the spreadsheet&lt;/a&gt; if you&amp;rsquo;d
like to look at it.
It&amp;rsquo;s quite clear from the chart&amp;rsquo;s context that the X-values should actually be
1, 5, 10 and 20 so those are easy corrections to make.
Using this technique I estimate the points as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1   36250
 5  110000
10  195000
20  313750
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A similar technique is to use &lt;a href=&#34;https://www.desmos.com&#34;&gt;Desmos&lt;/a&gt;. You can insert
an image, set its dimensions equal to its pixel count, then create a table and
use the gear menu to enable dragging on the table&amp;rsquo;s points, turning on the 4-way
drag handles. Then align points over the chart&amp;rsquo;s points, and you can read off
the values from the table just as with the Preview app:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.xaprb.com/media/2015/12/using-desmos.jpg&#34; alt=&#34;Using Desmos&#34; /&gt;&lt;/p&gt;

&lt;p&gt;One nice thing about this method is that you can then combine the image with any
graphs you&amp;rsquo;re making. For example, I used this method to facetiously analyze the
&lt;a href=&#34;http://www.vividcortex.com/blog/2015/11/28/a-trendline-is-a-model/&#34;&gt;path of the eclipsing
moon&lt;/a&gt; on a
recent blog post.&lt;/p&gt;

&lt;h3 id=&#34;easier-ways&#34;&gt;Easier Ways&lt;/h3&gt;

&lt;p&gt;Why use hard methods like that? I believe the hard ways are still valuable to
know about. First of all, when you do it you get a firm grasp on the math and
you&amp;rsquo;re tuned in to what you&amp;rsquo;re working on (or I am, at least). Second, sometimes
when charts are skewed by, say, the camera&amp;rsquo;s perspective, you have to estimate
where the points would fall if the skew were corrected.&lt;/p&gt;

&lt;p&gt;That said, there are a few tools that are easier to use and produce good
results. While searching online I found
&lt;a href=&#34;http://digitizer.sourceforge.net/&#34;&gt;engauge&lt;/a&gt;, &lt;a href=&#34;http://www.datathief.org/&#34;&gt;Data
Thief&lt;/a&gt;, and &lt;a href=&#34;di8itapp&#34;&gt;http://di8itapp.com/&lt;/a&gt;. But the
best one I&amp;rsquo;ve found so far is the free online &lt;a href=&#34;http://arohatgi.info/WebPlotDigitizer/app/&#34;&gt;web plot
digitizer&lt;/a&gt;, which runs in the
browser and produced quite good results for me. It allows very fine control over
the placement of the points. The extracted points are:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 0.995104673   32858.00211
 5.070422535  108307.8542
10.07535128   196097.94
20.17810302   312738.7098
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course, the X-points should be set to 1, 5, 10, and 20 as before.&lt;/p&gt;

&lt;p&gt;As a bonus, this app integrates with &lt;a href=&#34;https://plot.ly&#34;&gt;Plotly&lt;/a&gt;, and can
automatically send the resulting data points there for analysis. Plotly is a
tool I wasn&amp;rsquo;t aware of previously. I found it quite nice for this type of
analysis and was able to quickly run a regression against the USL and estimate
the coefficient of serialization at 4% and the coefficient of crosstalk at 0,
which is very realistic for this type of system in my experience:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.xaprb.com/media/2015/12/plotly.jpg&#34; alt=&#34;Plotly&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That was easy! Easier than using RStudio, perhaps.&lt;/p&gt;

&lt;h3 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;Using the techniques I showed in this article you can extract graph points from
a variety of different images when you lack the source data. Some of these
techniques are easier to use on large datasets than others, and some are just
more fun if you feel like doing things manually, but all can produce results
good enough for many purposes.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re curious about the analysis of CitusDB, and what it means for it to
have a coefficient of serialization (sigma) of about 4%, please read my
&lt;a href=&#34;https://www.vividcortex.com/resources/universal-scalability-law/&#34;&gt;introduction to the Universal Scalability
Law&lt;/a&gt;. If
you&amp;rsquo;re like me, you&amp;rsquo;ll find it fascinating that such a model exists. (Exercise
for the reader: what&amp;rsquo;s the maximum speedup CitusDB should be able to achieve as
you add more nodes to it?)&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.flickr.com/photos/schill/14418736104/&#34;&gt;Photo by Scott Schiller on Flickr&lt;/a&gt;.&lt;/p&gt;</description>
        </item>
    
        <item>
          <title>Thinking clearly about fitting a model to data</title>
          <link>https://www.xaprb.com/blog/2013/01/31/thinking-clearly-about-fitting-a-model-to-data/</link>
          <pubDate>Thu, 31 Jan 2013 00:00:00 UTC</pubDate>
          <author></author>
          <guid>https://www.xaprb.com/blog/2013/01/31/thinking-clearly-about-fitting-a-model-to-data/</guid>
          <description>&lt;p&gt;I have often seen people fitting curves to sets of data without first understanding whether that is appropriate. I once even &lt;a href=&#34;https://www.xaprb.com/blog/2011/01/15/sleep-while-you-can-because-it-wont-last-long/&#34; title=&#34;Sleep while you can, because it won’t last long&#34;&gt;used this blog&lt;/a&gt; to criticize someone for doing that.&lt;/p&gt;

&lt;p&gt;I was trying to explain that it&amp;rsquo;s wrong to fit a model to a set of measurements, unless the model actually describes the process that produced the measurements.&lt;/p&gt;

&lt;p&gt;All of my explanations (and rants) have fallen far short of the clarity and simplicity of this &lt;a href=&#34;http://graphpad.com/guides/prism/6/curve-fitting/&#34;&gt;curve-fitting guide&lt;/a&gt;. It&amp;rsquo;s a user&amp;rsquo;s manual for the GraphPad software, but it&amp;rsquo;s really about curve-fitting in general. It is &lt;em&gt;excellent&lt;/em&gt; reading. Here are a few selected gems:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Choosing a model is a scientific decision. You should base your choice on your understanding of chemistry or physiology (or genetics, etc.). The choice should not be based solely on the shape of the graph&amp;hellip; Don&amp;rsquo;t use a computer program as a way to avoid understanding your experimental system, or to avoid making scientific decisions.&lt;/p&gt;

&lt;p&gt;Your goal in using a model is not necessarily to describe your system perfectly. A perfect model may have too many parameters to be useful. Rather, your goal is to find as simple a model as possible that comes close to describing your system. You want a model to be simple enough so you can fit the model to data, but complicated enough to fit your data well and give you parameters that help you understand the system, reach valid scientific conclusions, and design new experiments.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This software user&amp;rsquo;s manual is better than any textbook I&amp;rsquo;ve read. There&amp;rsquo;s no fluff, just common sense, uncommonly well-written.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll leave you with another example of curve-fitting that took place recently: &lt;a href=&#34;http://redmonk.com/dberkholz/2013/01/21/github-will-hit-5-million-users-within-a-year/&#34;&gt;predicting Github&amp;rsquo;s user growth&lt;/a&gt;. Does the author use a model that&amp;rsquo;s known to actually describe the processes at work in the growth of a population? A commenter makes a &lt;em&gt;great&lt;/em&gt; point: &amp;ldquo;I&amp;rsquo;d love to see what happens if you just fit to the first 3 years and chart out the modelled prediction vs reality for years 4 and 5.&amp;rdquo; That&amp;rsquo;s one of the fastest ways to tell whether you&amp;rsquo;re seeing things. All meaning has pattern, but not all pattern has meaning. That&amp;rsquo;s a Neil Gunther quote.&lt;/p&gt;

&lt;p&gt;Speaking of that, Neil Gunther counters with a &lt;a href=&#34;https://groups.google.com/d/topic/guerrilla-capacity-planning/Hd9SQy654c4/discussion&#34;&gt;power law model&lt;/a&gt;. He thinks Redmonk is wrong and it&amp;rsquo;ll take quite a bit longer for Github to hit 5 million users. My money&amp;rsquo;s on Neil Gunther.&lt;/p&gt;
</description>
        </item>
    
        <item>
          <title>A close look at New Relic&#39;s scalability chart</title>
          <link>https://www.xaprb.com/blog/2013/01/07/a-close-look-at-new-relics-scalability-chart/</link>
          <pubDate>Mon, 07 Jan 2013 00:00:00 UTC</pubDate>
          <author></author>
          <guid>https://www.xaprb.com/blog/2013/01/07/a-close-look-at-new-relics-scalability-chart/</guid>
          <description>&lt;p&gt;I&amp;rsquo;ve written a lot about modeling MySQL with the USL, and I like it best of all the scalability models I&amp;rsquo;ve seen, but it&amp;rsquo;s not the only way to think about scalability. I was aware that New Relic supports a scalability chart, so I decided to take a peek at that. Here&amp;rsquo;s a screenshot of the chart, from &lt;a href=&#34;http://blog.newrelic.com/2011/06/13/of-rainbows-and-polka-dots-new-relics-scalability-charts-explained/&#34;&gt;their blog&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&#34;blog-rpm-response1&#34; src=&#34;https://www.xaprb.com/media/2013/01/blog-rpm-response1.png&#34; width=&#34;510&#34; height=&#34;295&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s how it works. It plots response time (or database time, or CPU) as the dependent variable, versus throughput as the independent variable. There&amp;rsquo;s a line through it to indicate the general shape. Samples are charted as points in a scatter plot. The points are color-coded by the time of day. Outliers are automatically removed.&lt;/p&gt;

&lt;p&gt;The focus on response time is really good. That&amp;rsquo;s one of the things I like about New Relic. While most systems show people status counters, and imply that they have some deep insight and meaningfulness (there&amp;rsquo;s usually no meaning to be found in status counters!), New Relic is educating people about the importance of response time, or latency.&lt;/p&gt;

&lt;p&gt;But as I read through the blog posts about this chart, it struck me that there&amp;rsquo;s something a little odd about it. The problem, I realized, is that it plots throughput as the independent variable on the chart. But throughput isn&amp;rsquo;t an independent variable. Throughput is the system&amp;rsquo;s output under load, and depends on a) the load on the system, b) the system&amp;rsquo;s scalability. It&amp;rsquo;s a &lt;em&gt;dependent&lt;/em&gt; variable.&lt;/p&gt;

&lt;p&gt;In a chart like this, it would be even better to show the independent variable as the variable that one can really control: the concurrency or load on the system. By &amp;ldquo;load&amp;rdquo; I mean the usual definition: the amount of work waiting to be completed, i.e. the backlog; this is what a Unix load average measures.&lt;/p&gt;

&lt;p&gt;To explain a little more what I mean about throughput being dependent, not independent, here are a few ways to think about it:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;An independent variable should range from zero to infinity (negative numbers are unphysical in a situation like this, so we exclude that). Throughput has a very finite theoretical and practical upper bound, but concurrency can theoretically go to infinity as work arrives and doesn&amp;rsquo;t complete.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;An independent variable is the variable &lt;em&gt;you can control as an input parameter of a system under test&lt;/em&gt;. It&amp;rsquo;s dead-easy to achieve the desired concurrency for a benchmark or other test. It&amp;rsquo;s &lt;em&gt;amazingly&lt;/em&gt; difficult to manufacture a desired throughput for a benchmark, even in &amp;ldquo;easy&amp;rdquo; conditions. Computers are unruly beasts &amp;ndash; they are queueing systems, and random variations and dependencies cause throughput to fluctuate greatly. That&amp;rsquo;s because throughput is measured at the &lt;em&gt;output&lt;/em&gt; end of the system, after the queues inside the system have had their way with the input and introduced statistical fluctuations into it. It&amp;rsquo;s quite easy to generate a desired &lt;em&gt;arrival rate&lt;/em&gt; for a system under test, provided that you have an unbounded number of workers ready to keep submitting more requests as the system queues up and stalls existing workers, but arrivals are not the same as throughput :-) Any way you look at it, you can pick your concurrency and your arrival rate, but you really can&amp;rsquo;t pick your throughput reliably. Throughput is an effect, not a cause.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;An independent variable in a function must map to one and only one value of the dependent variable. But we know that as load increases, a system&amp;rsquo;s throughput rises, peaks, and then falls again as retrograde scalability manifests itself. Suppose a system&amp;rsquo;s throughput goes from 10,000 queries per second at 16 threads, to 20,000 at 32 threads, and back to 10,000 at 64 threads. Now if we flip the chart&amp;rsquo;s axes around and treat throughput as an input, we&amp;rsquo;ll find that a throughput of 10,000 queries per second would map to either 16 or 64 threads. That doesn&amp;rsquo;t describe a real function.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So although the New Relic scalability chart shows some of the &lt;em&gt;effects&lt;/em&gt; of the system&amp;rsquo;s scalability, and it&amp;rsquo;s great to visualize the variation in response time as throughput varies, it doesn&amp;rsquo;t strike me as quite the right angle of approach.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m curious to hear from people who may have used this feature. What did you use it for? Were you successful in gaining insight into scalability bottlenecks? How did it help you?&lt;/p&gt;
</description>
        </item>
    
        <item>
          <title>Modeling scalability with the USL at concurrencies less than 1</title>
          <link>https://www.xaprb.com/blog/2013/01/05/modeling-scalability-with-the-usl-at-concurrencies-less-than-1/</link>
          <pubDate>Sat, 05 Jan 2013 00:00:00 UTC</pubDate>
          <author></author>
          <guid>https://www.xaprb.com/blog/2013/01/05/modeling-scalability-with-the-usl-at-concurrencies-less-than-1/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;https://www.xaprb.com/blog/2013/01/03/determining-the-usls-coefficient-of-performance-part-2/&#34; title=&#34;Determining the USL’s coefficient of performance, part 2&#34;&gt;Last time&lt;/a&gt; I said that you can set a starting value for the USL&amp;rsquo;s coefficient of performance and let your modeling software (R, gnuplot, etc) manipulate this as part of the regression to find the best fit. However, there is a subtlety in the USL model that you need to be aware of. Here is a picture of the low-end of the curve:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.xaprb.com/media/2013/01/usl1.png&#34; alt=&#34;usl&#34; width=&#34;490&#34; height=&#34;486&#34; class=&#34;aligncenter size-full wp-image-3008&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The graph shows the USL model as the blue curve and linear scalability as the black line. Notice that at concurrencies less than 1, the value of the USL function is actually greater than the linear scalability function. This deserves some thought and explanation, because it can cause problems.&lt;/p&gt;

&lt;p&gt;If you think about it, concurrency between one and zero is impossible. In fact, concurrency is not a smooth function, it is a step function. There can be zero requests resident in the system, one request, two requests, and so on &amp;ndash; but not 0.7 requests or 3.14159 requests. However, the USL is defined in terms of a continuous function, not a step function.&lt;/p&gt;

&lt;p&gt;The trouble with the MySQL systems I usually model is that I generally observe them in the wild, which means that I get a large number of samples of throughput-and-concurrency, and I aggregate them. For example, I&amp;rsquo;ll usually observe concurrency once per second, and average these samples over a minute or more for each point I want to feed into the USL model. This approach generates concurrency values that are real numbers, not just integers &amp;ndash; so it&amp;rsquo;s entirely possible that during a given minute, the &amp;ldquo;average concurrency&amp;rdquo; on the system comes out to 0.7 or 3.14159. What&amp;rsquo;s to be done with this?&lt;/p&gt;

&lt;p&gt;In the perfect world, I&amp;rsquo;d like to delete &amp;ldquo;empty space&amp;rdquo; during which zero queries were executing, and determine the actual throughput at each integral value of concurrency. But it&amp;rsquo;s a lot less convenient to do this, at best; and it&amp;rsquo;s usually impractical or impossible. So I work with the data I have. In practice I find it&amp;rsquo;s good enough.&lt;/p&gt;

&lt;p&gt;Back to the funny anomaly where the USL predicts better-than-linear scalability between concurrency=0 and =1. The outcome is that the regression to the USL model can potentially skew the values of the USL coefficients, if you have any samples that lie between 0 and 1. Thus, it may be a good idea to discard these samples. This should not be a significant portion of your sample dataset anyway. If you don&amp;rsquo;t have a lot of samples at higher concurrencies, you probably don&amp;rsquo;t have enough data to model the system accurately, and you should act accordingly.&lt;/p&gt;
</description>
        </item>
    
        <item>
          <title>Determining the USL&#39;s coefficient of performance, part 2</title>
          <link>https://www.xaprb.com/blog/2013/01/03/determining-the-usls-coefficient-of-performance-part-2/</link>
          <pubDate>Thu, 03 Jan 2013 00:00:00 UTC</pubDate>
          <author></author>
          <guid>https://www.xaprb.com/blog/2013/01/03/determining-the-usls-coefficient-of-performance-part-2/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;https://www.xaprb.com/blog/2013/01/02/determining-the-universal-scalability-laws-coefficient-of-performance/&#34; title=&#34;Determining the Universal Scalability Law’s coefficient of performance&#34;&gt;Last time&lt;/a&gt; I said that the USL has a forgotten third coefficient, the coefficient of performance. This is the same thing as the system&amp;rsquo;s throughput at concurrency=1, or C(1). How do you determine this coefficient? There are at least three ways.&lt;/p&gt;

&lt;p&gt;Neil Gunther&amp;rsquo;s writings, or at least those that I&amp;rsquo;ve read and remember, say that you should set it equal to your measurement of C(1). Most of his writing discusses a handful of measurements of the system: one at concurrency 1, and at least 4 to 6 at higher concurrencies. I can&amp;rsquo;t remember a time when he&amp;rsquo;s discussed taking more than one measurement of throughput at each level of concurrency, so I think the assumption is that you&amp;rsquo;re going to take a single measurement at various concurrencies (or, in the case of hardware scalability, units of hardware), and you&amp;rsquo;re done.&lt;/p&gt;

&lt;p&gt;This tends to work quite well. I&amp;rsquo;ve blogged before about this: well-designed systems, measured in a carefully controlled test, tend to match the Universal Scalability Law model quite well. Here are &lt;a href=&#34;http://www.mysqlperformanceblog.com/2011/01/26/modeling-innodb-scalability-on-multi-core-servers/&#34;&gt;two&lt;/a&gt; &lt;a href=&#34;http://www.mysqlperformanceblog.com/2011/02/28/is-voltdb-really-as-scalable-as-they-claim/&#34;&gt;examples&lt;/a&gt;.
Most systems I model aren&amp;rsquo;t like that. I don&amp;rsquo;t do my modeling in a lab. I get thousands, if not tens or hundreds of thousands, of measurements of throughput and concurrency from a MySQL server&amp;rsquo;s real production traffic. How do you determine the system&amp;rsquo;s throughput at concurrency=1 in this kind of situation? You may have hundreds or thousands of samples at or near concurrency=1, and here&amp;rsquo;s the interesting thing: they aren&amp;rsquo;t tightly clustered. This leads to the two additional techniques I&amp;rsquo;ve used.&lt;/p&gt;

&lt;p&gt;Method 2 is fairly obvious: you can take an aggregate measure of the throughput at N=1. You can simply average, or you can use the median. In my experience, the latter tends to be a little more accurate, because the median essentially discards outliers. Given enough samples, it is very likely that the median is truly representative of the system&amp;rsquo;s real behavior.&lt;/p&gt;

&lt;p&gt;Finally, method 3 is to treat C(1) as one of the parameters to fit in the regression to the USL model. Instead of holding it as a fixed quantity, go ahead and let the regression find the best fit for it along with the other coefficients.&lt;/p&gt;

&lt;p&gt;In practice, I tend to combine methods 2 and 3. I use method 2 to find a starting point for the coefficient, and then I let the regression tweak it as needed. In my experience, this usually produces good results. Sometimes the software doing the regression gets a little confused, or stuck at a local maximum, but otherwise it works well.&lt;/p&gt;

&lt;p&gt;What if you don&amp;rsquo;t have measurements at N=1? The best approach, in my experience, is to take the slope of the line from the first data point you have, and use that. N=1 will almost always be higher than this, because real systems are rarely linearly scalable. That&amp;rsquo;s okay. If you let the regression adjust the coefficient as needed for the best fit, you&amp;rsquo;ll end up with a good answer anyway.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re interested in learning more, I wrote an &lt;a href=&#34;https://www.vividcortex.com/resources/universal-scalability-law/&#34;&gt;ebook about the Universal
Scalability Law&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
    
        <item>
          <title>Determining the Universal Scalability Law&#39;s coefficient of performance</title>
          <link>https://www.xaprb.com/blog/2013/01/02/determining-the-universal-scalability-laws-coefficient-of-performance/</link>
          <pubDate>Wed, 02 Jan 2013 00:00:00 UTC</pubDate>
          <author></author>
          <guid>https://www.xaprb.com/blog/2013/01/02/determining-the-universal-scalability-laws-coefficient-of-performance/</guid>
          <description>&lt;p&gt;If you&amp;rsquo;re familiar with Neil Gunther&amp;rsquo;s Universal Scalability Law, you may have heard it said that there are two coefficients, variously called alpha and beta or sigma and kappa. There are actually three coefficients, though. See?&lt;/p&gt;

&lt;p&gt;\[
C(N) = \frac{N}{1 + \sigma(N-1) + \kappa N (N-1)}
\]&lt;/p&gt;

&lt;p&gt;No, you don&amp;rsquo;t see it &amp;ndash; but it&amp;rsquo;s actually there, as a hidden 1 multiplied by N in the numerator on the right-hand side. When you&amp;rsquo;re using the USL to model a system&amp;rsquo;s scalability, you need to use the C(1), the &amp;ldquo;capacity at one,&amp;rdquo; as a multiplier. I call this the coefficient of performance. It&amp;rsquo;s rarely 1; it&amp;rsquo;s usually thousands.&lt;/p&gt;

&lt;p&gt;To illustrate why this matters, consider two systems&amp;rsquo; throughput as load increases:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.xaprb.com/media/2013/01/coeff-of-performance.png&#34; alt=&#34;coefficient of performance&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The green line and the blue line are both linearly scalable systems. Add twice the concurrency, get twice the throughput. But the slope of the lines is different. The blue system can do twice as much work as the green system, even though it&amp;rsquo;s no more scalable.&lt;/p&gt;

&lt;p&gt;To model the USL, you need to determine C(1) by measuring the system under test. In my experience with real systems running in production, mostly MySQL servers, this is not simple. You can&amp;rsquo;t just say &amp;ldquo;let&amp;rsquo;s quiet the web app down, I want to load it with exactly one user for a few minutes and measure how fast it runs.&amp;rdquo; Instead, you get a bunch of samples from production traffic, and you &lt;a href=&#34;https://www.xaprb.com/blog/2013/01/03/determining-the-usls-coefficient-of-performance-part-2/&#34;&gt;derive&lt;/a&gt; the throughput at concurrency=1 from that.&lt;/p&gt;

&lt;p&gt;The result goes into the numerator as a multiplier of N, although it&amp;rsquo;s usually omitted when the USL formula is shown.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re interested in learning more, I wrote an &lt;a href=&#34;https://www.vividcortex.com/resources/universal-scalability-law/&#34;&gt;ebook about the Universal
Scalability Law&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
    
        <item>
          <title>Scalability, performance, capacity planning and USL at Hotsos Symposium</title>
          <link>https://www.xaprb.com/blog/2012/03/07/scalability-performance-capacity-planning-and-usl-at-hotsos-symposium/</link>
          <pubDate>Wed, 07 Mar 2012 00:00:00 UTC</pubDate>
          <author></author>
          <guid>https://www.xaprb.com/blog/2012/03/07/scalability-performance-capacity-planning-and-usl-at-hotsos-symposium/</guid>
          <description>&lt;p&gt;I presented at this year&amp;rsquo;s [Hotsos Symposium](). I am searching for a claim to specialness, and I think it may be that I am the first Hotsos presenter who&amp;rsquo;s specifically focused on MySQL. True? I don&amp;rsquo;t know, but I&amp;rsquo;ll run with it for now.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://vividcortex.com/&#34;&gt;VividCortex&lt;/a&gt; is the startup I founded in 2012. It&amp;rsquo;s the easiest way to monitor what
your servers are doing in production. It does TCP network
traffic analysis. VividCortex offers &lt;a href=&#34;https://vividcortex.com/monitoring/mysql/&#34;&gt;MySQL performance
monitoring&lt;/a&gt; and &lt;a href=&#34;https://vividcortex.com/monitoring/postgres/&#34;&gt;PostgreSQL
performance management&lt;/a&gt; among many
other features.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;My topic was on extracting black-box performance metrics from TCP packet headers and timestamps and finding hidden performance problems in the system, without any knowledge of what the client and server are talking to each other about. I then extended the same data to performance and scalability modeling, which you can use for purposes such as forecasting, capacity planning, and bottleneck analysis.&lt;/p&gt;

&lt;p&gt;This technique works on MySQL because its TCP protocol is half-duplex, and it&amp;rsquo;ll work for any system with a half-duplex protocol. Does it work on Oracle Database? I am not sure, and no one else I&amp;rsquo;ve spoken to yet has been certain either. I can probably find out with a little research into the Oracle Database protocol.&lt;/p&gt;

&lt;p&gt;I wrote a white paper that goes into my presentation topics in more details. You can find it [here](), along with sample data and commands that you can use to reproduce my results. This covers Part I of my presentation, and I will publish another white paper with Part II in a while; probably after the [MySQL conference]() in April.&lt;/p&gt;

&lt;p&gt;My techniques are based on models and approaches that [Neil Gunther]() developed, and Neil himself presented just after I did. His talk was about power-law distributions, and how a log-log plot renders a power-law relationship as linear. It turns out that power laws are related to fractals, the coastline of Britain, the frequency of word usage in the English language, and [response time in Oracle workloads](). I&amp;rsquo;m sure he will post some details on his [blog]().&lt;/p&gt;

&lt;p&gt;After the day&amp;rsquo;s sessions ended, I ended up talking to Neil for a while. He explained and clarified a lot of things I didn&amp;rsquo;t understand about his work, such as the relationship between repairman queueing and the Universal Scalability Law. He also saw through a variety of my misconceptions and set me straight. Apparently I need to attend one of his training classes. We followed this by eating dinner and sharing a bottle of wine until late in the evening, and Neil wouldn&amp;rsquo;t let me pay in the end. A most enjoyable meal and conversation. Next time it&amp;rsquo;s on me!&lt;/p&gt;
</description>
        </item>
    
        <item>
          <title>Fundamental performance and scalability instrumentation</title>
          <link>https://www.xaprb.com/blog/2011/10/06/fundamental-performance-and-scalability-instrumentation/</link>
          <pubDate>Thu, 06 Oct 2011 00:00:00 UTC</pubDate>
          <author></author>
          <guid>https://www.xaprb.com/blog/2011/10/06/fundamental-performance-and-scalability-instrumentation/</guid>
          <description>&lt;p&gt;This post is a followup to some promises I made at Postgres Open.&lt;/p&gt;

&lt;p&gt;Instrumentation can be a lot of work to add to a server, and it can add overhead to the server too. The bits of instrumentation I&amp;rsquo;ll advocate in this post are few and trivial, but disproportionately powerful.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: &lt;a href=&#34;https://vividcortex.com/&#34;&gt;VividCortex&lt;/a&gt; is the startup I founded in 2012. It&amp;rsquo;s the easiest way to monitor what
your servers are doing in production. VividCortex offers &lt;a href=&#34;https://vividcortex.com/monitoring/mysql/&#34;&gt;MySQL performance
monitoring&lt;/a&gt; and &lt;a href=&#34;https://vividcortex.com/monitoring/postgres/&#34;&gt;PostgreSQL
performance management&lt;/a&gt; among many
other features.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If all server software shipped with these metrics as the basic starting point, it would change the world forever:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Time elapsed, in high resolution (preferably microseconds; milliseconds is okay; one-second is mostly useless). When I ask for this counter, it simply tells me either the time of day, or the server&amp;rsquo;s uptime, or something like that. It can be used to determine the boundaries of an observation interval, defined by two measurements. It needs to be consistent with the other metrics that I&amp;rsquo;ll explain next.&lt;/li&gt;
&lt;li&gt;The number of queries (statements) that have completed.&lt;/li&gt;
&lt;li&gt;The current number of queries being executed.&lt;/li&gt;
&lt;li&gt;The total execution time of all queries, including the in-progress time of currently executing queries, in high resolution. That is, if two queries executed with 1 second of response time each, the result is 2 seconds, no matter whether the queries executed concurrently or serially. If one query started executing .5 seconds ago and is still executing, it should contribute .5 second to the counter.&lt;/li&gt;
&lt;li&gt;The server&amp;rsquo;s total busy time, in high resolution. This is different from the previous point in that it only shows the portion of the observation interval during which queries were executing, regardless of whether they were concurrent or not. If two queries with 1-second response time executed serially, the counter is 2. If they executed concurrently, the counter is something less than 2, because the overlapping time isn&amp;rsquo;t double-counted.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In practice, these can be maintained as follows, in pseudo-code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global timestamp;
global concurrency;
global busytime;
global totaltime;
global queries;

function run_query() {
  local now = time();
  if ( concurrency ) {
    busytime += now - timestamp;
    totaltime += (now - timestamp) * concurrency;
  }
  concurrency++;
  timestamp = now;

  // Execute the query, and when it completes...

  now = time();
  busytime += now - timestamp;
  totaltime += (now - timestamp) * concurrency;
  concurrency--;
  timestamp = now;
  queries++;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I may have missed something there; I&amp;rsquo;m writing this off the cuff. If I&amp;rsquo;ve messed up, let me know and I&amp;rsquo;ll fix it. In any case, these metrics can be used to derive all sorts of powerful things through applications of Little&amp;rsquo;s Law and queueing theory, as well as providing the inputs to the Universal Scalability Law. They should be reported by simply reading from the variables marked as &amp;ldquo;global&amp;rdquo; above, to provide a consistent view of the metrics.&lt;/p&gt;
</description>
        </item>
    
        <item>
          <title>Using BASE instead of ACID for scalability</title>
          <link>https://www.xaprb.com/blog/2008/07/23/using-base-instead-of-acid-for-scalability/</link>
          <pubDate>Wed, 23 Jul 2008 00:00:00 UTC</pubDate>
          <author></author>
          <guid>https://www.xaprb.com/blog/2008/07/23/using-base-instead-of-acid-for-scalability/</guid>
          <description>&lt;p&gt;My editor &lt;a href=&#34;http://www.oreillynet.com/pub/au/36&#34;&gt;Andy Oram&lt;/a&gt; recently sent me an &lt;a href=&#34;http://acmqueue.com/modules.php?name=Content&amp;amp;#038;pa=showpage&amp;amp;#038;pid=540&#34;&gt;ACM article on BASE, a technique for improving scalability&lt;/a&gt; by being willing to give up some other properties of traditional transactional systems.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s a really good read. In many ways it is the same religion everyone who&amp;rsquo;s successfully scaled a system Really Really Big has advocated. But this is different: it&amp;rsquo;s a very clear article, with a great writing style that really cuts out the fat and teaches the principles without being specific to any environment or sounding egotistical.&lt;/p&gt;

&lt;p&gt;He mentions a lot of current thinking in the field, including the CAP principle, which &lt;a href=&#34;http://www.continuent.com/&#34;&gt;Robert Hodges of Continuent&lt;/a&gt; first turned me onto a couple months ago. &lt;a href=&#34;http://citeseer.ist.psu.edu/544596.html&#34;&gt;It has been proven formally&lt;/a&gt;, though I have not read the proof myself.&lt;/p&gt;

&lt;p&gt;One of the most important concepts he advances is giving up the illusion of control. As programmers and DBAs, I think we may tend to like control too much. Foreign keys are a perfect example. I think the point here is that these things make you feel safe, but they don&amp;rsquo;t really make you safe. Just as with so many things in life, recognizing our inability to really control the systems we build is key to working with their strengths &amp;ndash; instead of trying to bind them with iron bands.&lt;/p&gt;

&lt;p&gt;Another great point is idempotency. This is a great way to help avoid problems with MySQL replication, by the way. I&amp;rsquo;ll leave the &amp;ldquo;why&amp;rdquo; as an exercise for the reader, but let me just point out that the file MySQL uses to remember its current position in replication is not synced to disk, so it will almost certainly get out of whack if MySQL dies ungracefully. (Google has solved this problem.)&lt;/p&gt;

&lt;p&gt;A highly recommended read &amp;ndash; worth more than most case studies about how specific companies have scaled their specific systems.&lt;/p&gt;
</description>
        </item>
    

  </channel>
</rss>
